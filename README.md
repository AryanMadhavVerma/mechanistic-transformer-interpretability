## Mechanistic interpretability analysis 
of a gpt style transformer, tinkering around with internal activation layers and target ablations to understand how attention heads, MLP neurons and layers causlaly contirbute to behaviour 

also building a gpt from scratch before starting experiments
